{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a97e24b",
   "metadata": {},
   "source": [
    "# Cut Columns\n",
    "This notebook is designed to reformat the column order and number of variables so that the training and predict data sets used with a given SuperLearner model are consistent. This allows the ML model to be reused across multiple predict data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc6ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59fbd85",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d33b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (79) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Initial Training data\n",
    "training_data_df = pd.read_csv('../../sbir-learnerworks-doc/notebooks/whondrml_all.csv')\n",
    "\n",
    "# Predict data\n",
    "predict_data_df = pd.read_csv('../../sbir-learnerworks-doc/GLORICH_HydroSHEDS/step_12_output.csv')\n",
    "\n",
    "# WHONDRS data updates\n",
    "update_training_df = pd.read_csv('../../sbir-learnerworks-doc/GLORICH_HydroSHEDS/step_11_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b0a1b",
   "metadata": {},
   "source": [
    "# Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ea11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any WHONDRS metadata not available in global training data.\n",
    "# Also remove any features we know we don't want (e.g. lon,lat).\n",
    "\n",
    "training_data_df.drop(columns=[\n",
    "    'US_Latitude_dec.deg',\n",
    "    'US_Water.Column.Height_cm',\n",
    "    'US_Algal.Mat.Coverage',\n",
    "    'US_Macrophyte.Coverage',\n",
    "    'US_Sunlight.Access_Perc.Canopy.Cover',\n",
    "    'Stream_Order',\n",
    "    'NOSC',\n",
    "    'lamO2',\n",
    "    'lamHCO3',\n",
    "    'n_chems',\n",
    "    'skew_lamO2',\n",
    "    'skew_lamHCO3'],inplace=True)\n",
    "\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"US_Depositional.Type_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"Sediment_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"General_Vegetation_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"perc_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"del_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"Hydrogeomorphology_\" in x],inplace=True)\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"River_Gradient_\" in x],inplace=True)\n",
    "\n",
    "# Process the update_training data so that it has the same columns as the original training data.\n",
    "# Note some additional steps are required below.\n",
    "# TO DO: replace the steps here and the steps below with a bash script for automated production.\n",
    "update_training_df.drop(columns=[\n",
    "    'GL_id',\n",
    "    'GL_lon',\n",
    "    'GL_lat',\n",
    "    'STAT_ID',\n",
    "    'TOC',\n",
    "    'RA_lm',\n",
    "    'my_lm',\n",
    "    'RA_lon',\n",
    "    'RA_lat',\n",
    "    'STAT_ID',\n",
    "    'dist_m'],inplace=True)\n",
    "\n",
    "# In the process of dropping lon,lat from predictions,\n",
    "# store them for plotting later.\n",
    "xy_df = pd.DataFrame()\n",
    "xy_df['GL_id'] = predict_data_df.pop('GL_id')\n",
    "xy_df['lon'] = predict_data_df.pop('GL_lon')\n",
    "xy_df['lat'] = predict_data_df.pop('GL_lat')\n",
    "\n",
    "# Drop variables that we will not use in the ML from the predict data.\n",
    "predict_data_df.drop(columns=[\n",
    "    'RA_lm',\n",
    "    'my_lm',\n",
    "    'RA_lon',\n",
    "    'RA_lat',\n",
    "    'STAT_ID',\n",
    "    'dist_m',\n",
    "    'TOC'\n",
    "    ],inplace=True)\n",
    "\n",
    "# Note here and below we shift to amplitude of stream flow change.\n",
    "predict_data_df['RA_ms_av'] = predict_data_df['RA_cms_cyr']/predict_data_df['RA_xam2']\n",
    "predict_data_df['RA_ms_di'] = (predict_data_df['RA_cms_cmx'] - predict_data_df['RA_cms_cmn'])/predict_data_df['RA_xam2']\n",
    "\n",
    "update_training_df['RA_ms_av'] = update_training_df['RA_cms_cyr']/update_training_df['RA_xam2']\n",
    "update_training_df['RA_ms_di'] = (update_training_df['RA_cms_cmx'] - update_training_df['RA_cms_cmn'])/update_training_df['RA_xam2']\n",
    "\n",
    "predict_data_df.drop(columns=[\n",
    "    'RA_cms_cyr',\n",
    "    'RA_cms_cmn',\n",
    "    'RA_cms_cmx',\n",
    "    'RA_xam2'\n",
    "    ],inplace=True)\n",
    "\n",
    "update_training_df.drop(columns=[\n",
    "    'RA_cms_cyr',\n",
    "    'RA_cms_cmn',\n",
    "    'RA_cms_cmx',\n",
    "    'RA_xam2'\n",
    "    ],inplace=True)\n",
    "\n",
    "# Reorder some columns\n",
    "training_data_df['Temp_water'] = training_data_df.pop('SW_Temp_degC')\n",
    "training_data_df['pH'] = training_data_df.pop('SW_pH')\n",
    "training_data_df['DO_mgL'] = training_data_df.pop('DO_mg.per.L')\n",
    "training_data_df['DOSAT'] = training_data_df.pop('DO_perc.sat')\n",
    "training_data_df['RA_ms_av'] = training_data_df.pop('RA_ms_av')\n",
    "training_data_df['RA_ms_di'] = training_data_df.pop('RA_ms_mx') - training_data_df.pop('RA_ms_mn')\n",
    "training_data_df['rate.mg.per.L.per.h'] = training_data_df.pop('rate.mg.per.L.per.h')\n",
    "\n",
    "update_training_df['rate.mg.per.L.per.h'] = update_training_df.pop('rate.mg.per.L.per.h')\n",
    "\n",
    "# Append the training data to the update (that way we can keep the data sets separate for now)\n",
    "update_training_df = update_training_df.append(training_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e453a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both training and predict data are consistent, \n",
    "# but need to remove more features for clarity.\n",
    "\n",
    "# Get rid of any upstream info\n",
    "training_data_df.drop(columns=[x for x in training_data_df.columns if \"_u\" in x],inplace=True)\n",
    "predict_data_df.drop(columns=[x for x in predict_data_df.columns if \"_u\" in x],inplace=True)\n",
    "update_training_df.drop(columns=[x for x in update_training_df.columns if \"_u\" in x],inplace=True)\n",
    "\n",
    "# All the anthropogenic influencers are correlated, so pick one for now:\n",
    "drop_anthro_features = ['pop_ct_csu','urb_pc_cse','nli_ix_cav','rdd_mk_cav','hft_ix_c09','gdp_md_cav','hdi_ix_cav']\n",
    "training_data_df.drop(columns=drop_anthro_features,inplace=True)\n",
    "predict_data_df.drop(columns=drop_anthro_features,inplace=True)\n",
    "update_training_df.drop(columns=drop_anthro_features,inplace=True)\n",
    "\n",
    "# Get rid of some more variables\n",
    "drop_features = [\n",
    "    'pac_pc_cse',\n",
    "    'cly_pc_cav',\n",
    "    'slt_pc_cav',\n",
    "    'snd_pc_cav',\n",
    "    'soc_th_cav',\n",
    "    'kar_pc_cse',\n",
    "    'ero_kh_cav',\n",
    "    'swc_pc_cyr',\n",
    "    'swc_pc_cdi',\n",
    "    'aet_mm_cyr',\n",
    "    'aet_mm_cdi',\n",
    "    'cmi_ix_cyr',\n",
    "    'cmi_ix_cdi',\n",
    "    'snw_pc_cyr',\n",
    "    'snw_pc_cmx',\n",
    "]\n",
    "training_data_df.drop(columns=drop_features,inplace=True)\n",
    "predict_data_df.drop(columns=drop_features,inplace=True)\n",
    "update_training_df.drop(columns=drop_features,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b4cf2",
   "metadata": {},
   "source": [
    "# Check column order (essential for SuperLearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b7f5ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  RA_SO  RA_SO  RA_SO\n",
      "1  RA_dm  RA_dm  RA_dm\n",
      "2  run_mm_cyr  run_mm_cyr  run_mm_cyr\n",
      "3  dor_pc_pva  dor_pc_pva  dor_pc_pva\n",
      "4  gwt_cm_cav  gwt_cm_cav  gwt_cm_cav\n",
      "5  ele_mt_cav  ele_mt_cav  ele_mt_cav\n",
      "6  slp_dg_cav  slp_dg_cav  slp_dg_cav\n",
      "7  sgr_dk_rav  sgr_dk_rav  sgr_dk_rav\n",
      "8  tmp_dc_cyr  tmp_dc_cyr  tmp_dc_cyr\n",
      "9  tmp_dc_cdi  tmp_dc_cdi  tmp_dc_cdi\n",
      "10  pre_mm_cyr  pre_mm_cyr  pre_mm_cyr\n",
      "11  pre_mm_cdi  pre_mm_cdi  pre_mm_cdi\n",
      "12  for_pc_cse  for_pc_cse  for_pc_cse\n",
      "13  crp_pc_cse  crp_pc_cse  crp_pc_cse\n",
      "14  pst_pc_cse  pst_pc_cse  pst_pc_cse\n",
      "15  ire_pc_cse  ire_pc_cse  ire_pc_cse\n",
      "16  gla_pc_cse  gla_pc_cse  gla_pc_cse\n",
      "17  prm_pc_cse  prm_pc_cse  prm_pc_cse\n",
      "18  ppd_pk_cav  ppd_pk_cav  ppd_pk_cav\n",
      "19  Temp_water  Temp_water  Temp_water\n",
      "20  pH  pH  pH\n",
      "21  DO_mgL  DO_mgL  DO_mgL\n",
      "22  DOSAT  DOSAT  DOSAT\n",
      "23  RA_ms_av  RA_ms_av  RA_ms_av\n",
      "24  RA_ms_di  RA_ms_di  RA_ms_di\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "for col in predict_data_df.columns:\n",
    "    print(str(ii)+'  '+col+'  '+training_data_df.columns[ii]+'  '+update_training_df.columns[ii])\n",
    "    ii = ii + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe89b654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GL_id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300043</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>41.135430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300086</td>\n",
       "      <td>0.092954</td>\n",
       "      <td>41.680723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300022</td>\n",
       "      <td>-0.122750</td>\n",
       "      <td>41.927435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300023</td>\n",
       "      <td>-0.131028</td>\n",
       "      <td>41.918934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300153</td>\n",
       "      <td>0.144293</td>\n",
       "      <td>41.122866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10524</th>\n",
       "      <td>5</td>\n",
       "      <td>-68.445497</td>\n",
       "      <td>44.640715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10525</th>\n",
       "      <td>6</td>\n",
       "      <td>-76.884050</td>\n",
       "      <td>37.571283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10526</th>\n",
       "      <td>7</td>\n",
       "      <td>-88.655746</td>\n",
       "      <td>33.430906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10527</th>\n",
       "      <td>8</td>\n",
       "      <td>-88.433519</td>\n",
       "      <td>33.453633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10528</th>\n",
       "      <td>9</td>\n",
       "      <td>-88.776194</td>\n",
       "      <td>33.289214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10529 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GL_id        lon        lat\n",
       "0      300043   0.002154  41.135430\n",
       "1      300086   0.092954  41.680723\n",
       "2      300022  -0.122750  41.927435\n",
       "3      300023  -0.131028  41.918934\n",
       "4      300153   0.144293  41.122866\n",
       "...       ...        ...        ...\n",
       "10524       5 -68.445497  44.640715\n",
       "10525       6 -76.884050  37.571283\n",
       "10526       7 -88.655746  33.430906\n",
       "10527       8 -88.433519  33.453633\n",
       "10528       9 -88.776194  33.289214\n",
       "\n",
       "[10529 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4293992",
   "metadata": {},
   "source": [
    "# Write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e3852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.to_csv('whondrml_global_train_25_inputs.csv',index=False,na_rep='NaN')\n",
    "update_training_df.to_csv('whondrml_global_train_25_inputs_update.csv',index=False,na_rep='NaN')\n",
    "predict_data_df.to_csv('WH_RA_GL_global_predict_25_inputs.csv',index=False,na_rep='NaN')\n",
    "xy_df.to_csv('WH_RA_GL_global_predict_ixy.csv',index=False,na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db34ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
